dataset_params:
  input_data_dir: '/glade/derecho/scratch/asheshc/ERA5_t30/train'
  output_data_dir: '/glade/derecho/scratch/mdarman/ERA5_hr_haiwen/data'
  year_range_start: 2000
  year_range_end: 2009
  normalize: True
  input_normalization_dir: '/glade/derecho/scratch/mdarman/lucie/stats_lr_2000_2009_updated.npz'
  output_normalization_dir: '/glade/derecho/scratch/mdarman/lucie/stats_hr_2000_2009_updated.npz'
  cache_file: '/glade/derecho/scratch/mdarman/lucie/valid_files.npz'
  force_recompute: False
  lucie_file_path: '/glade/derecho/scratch/mdarman/lucie/LUCIE_inference_start2010.npz'
  input_channels: 8
  output_channels: 4
  
  land_sea_mask: 0 # 1 for True, 0 for False
  land_sea_mask_dir: '/glade/derecho/scratch/mdarman/lucie/lsm.npz'
  name: 'celebhq'

diffusion_params:
  num_timesteps : 1000
  beta_start : 0.0001
  beta_end : 0.02

fno_params:
  modes1: 180
  modes2: 360
  width: 20

ddpm_fno_params:
  modes1: 180
  modes2: 360
  width: 20
  time_emb_dim: 512

train_params:
  # General training settings
  seed: 1111  # Random seed for reproducibility
  task_name: 'lazy_diff_v2'  # Name of the current training task
  description : 'Training lazy diff using spectral loss of lambda0p5'  # Description of the training task
  results_dir: '/glade/derecho/scratch/mdarman/lucie/results'  # Directory to save results: task_dir = os.path.join(results_dir, train_config['task_name'])
  load_dir: '/glade/derecho/scratch/mdarman/lucie/results'  # Directory to load results from
  load_from_checkpoint: False  # Whether to load the model from a previous checkpoint
  fno_resume_ckpt_path: '/glade/derecho/scratch/mdarman/lucie/results/fno_v2/checkpoints/latest_epoch.pth'  # Path to the model checkpoint

  # Batch sizes and memory-related configurations
  ldm_batch_size: 2  # Batch size for latent diffusion model (LDM) training (limited by GPU capacity: for Derecho 1 per GPU)
  fno_batch_size: 8  # Batch size for FNO training (limited by GPU capacity: for Derecho 4 per GPU) 
  
  # Weights for other loss components
  lambda_fft: 0.5  # Weight for FFT loss

  # Training duration
  ldm_epochs: 100  # Number of epochs for training the latent diffusion model (LDM)
  fno_epochs: 100  # Number of epochs for training the FNO model

  # Learning rates for different models
  ldm_lr: 0.0001  # Learning rate for the latent diffusion model (LDM)
  fno_lr: 0.001  # Learning rate for the FNO model

  # Scheduler settings for learning rate decay
  scheduler_step_size_g: 10  # Step size for learning rate scheduler (generator)
  scheduler_gamma_g: 0.90  # Gamma for learning rate decay (generator)
  
  # Gradient accumulation settings
  fno_accumulation_steps: 1  # Number of steps to accumulate gradients for autoencoder

  # Checkpoint and saving settings
  ldm_save_interval: 10  # Interval (in epochs) to save LDM checkpoints
  checkpoint_interval: 10  # Interval (in epochs) to save model checkpoints

  # Checkpoint file names
  ldm_ckpt_name: 'ddpm_ckpt.pth'  # Checkpoint file name for LDM
  fno_ckpt_name: 'fno_ckpt.pth'  # Checkpoint file name for FNO model
